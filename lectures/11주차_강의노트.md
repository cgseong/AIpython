## Scikit-learn 기초 강의노트

---

### 학습목표
*   Scikit-learn 라이브러리의 역할과 중요성을 이해한다.
*   Scikit-learn의 주요 모듈과 기본 API 사용법을 익힌다.
*   대표적인 머신러닝 프로세스(데이터 준비, 모델 학습, 예측, 평가)를 Scikit-learn으로 구현할 수 있다.
*   간단한 분류(Classification) 및 회귀(Regression) 문제를 Scikit-learn을 활용하여 해결할 수 있다.

---

### 💡 핵심개념
1.  **Scikit-learn (사이킷런)**: 파이썬 기반의 오픈소스 머신러닝 라이브러리. 다양한 분류, 회귀, 클러스터링, 차원 축소, 모델 선택, 전처리 알고리즘을 제공.
2.  **Estimator (추정기)**: Scikit-learn의 핵심 객체. 데이터를 학습하고 예측하는 모든 알고리즘 클래스를 지칭. 일관된 API (`fit()`, `predict()`, `transform()`)를 제공.
    *   `fit(X, y)`: 모델 학습 (지도학습의 경우 X: 특성, y: 레이블)
    *   `predict(X_new)`: 새로운 데이터에 대한 예측
    *   `transform(X)`: 데이터 변환 (주로 전처리 단계에서 사용)
    *   `fit_transform(X, y)`: `fit`과 `transform`을 동시에 수행
3.  **데이터셋 (Dataset)**: 머신러닝 모델 학습 및 평가에 사용되는 데이터. Scikit-learn은 예제 데이터셋을 제공 (`sklearn.datasets`).
4.  **특성 (Features)**: 입력 데이터(X). 독립 변수. 일반적으로 2차원 배열 형태 (샘플 수, 특성 수).
5.  **레이블 (Labels) / 타겟 (Target)**: 예측하고자 하는 값(y). 종속 변수. 분류에서는 범주형, 회귀에서는 연속형. 일반적으로 1차원 배열 형태 (샘플 수,).
6.  **훈련 세트 (Training Set) / 테스트 세트 (Test Set)**: 모델 학습에 사용되는 데이터와 모델 성능 평가에 사용되는 데이터를 분리한 것 (`sklearn.model_selection.train_test_split`).
7.  **모델 평가 (Model Evaluation)**: 학습된 모델의 성능을 측정하는 과정. 분류에서는 정확도(accuracy), 정밀도(precision), 재현율(recall) 등. 회귀에서는 평균 제곱 오차(MSE), R-squared 등 (`sklearn.metrics`).

---

### 🔍 이론 설명

#### 1. Scikit-learn 이란?
Scikit-learn은 파이썬으로 작성된 가장 인기 있는 머신러닝 라이브러리 중 하나입니다. NumPy, SciPy, Matplotlib을 기반으로 하며, 사용하기 쉬운 API와 풍부한 문서로 초보자부터 전문가까지 널리 사용됩니다.

**주요 특징:**
*   **다양한 알고리즘**: 분류, 회귀, 군집, 차원 축소 등 다양한 최신 머신러닝 알고리즘 제공.
*   **일관된 API**: 모든 Estimator 객체는 `fit()`, `predict()` 등의 유사한 메서드를 가져 사용이 편리.
*   **데이터 전처리 및 특성 공학**: 스케일링, 인코딩, 특성 선택 등 다양한 기능 제공.
*   **모델 선택 및 평가**: 교차 검증, 하이퍼파라미터 튜닝, 성능 지표 계산 기능 제공.
*   **오픈 소스**: BSD 라이선스로 누구나 자유롭게 사용 및 수정 가능.

#### 2. Scikit-learn의 기본 사용 흐름
일반적인 머신러닝 프로젝트는 다음과 같은 흐름을 따릅니다.

1.  **데이터 준비 (Data Preparation)**:
    *   데이터 불러오기 (`sklearn.datasets` 또는 외부 파일).
    *   데이터 탐색 및 전처리 (결측치 처리, 이상치 제거, 특성 스케일링, 범주형 데이터 인코딩 등).
2.  **데이터 분리 (Data Splitting)**:
    *   훈련 데이터와 테스트 데이터로 분리 (`train_test_split`).
3.  **모델 선택 (Model Selection)**:
    *   문제 유형(분류, 회귀 등)에 맞는 적절한 알고리즘 선택.
4.  **모델 학습 (Model Training)**:
    *   선택한 모델 객체를 생성하고, `fit()` 메서드를 사용하여 훈련 데이터로 모델을 학습.
5.  **모델 예측 (Model Prediction)**:
    *   학습된 모델의 `predict()` 메서드를 사용하여 테스트 데이터 또는 새로운 데이터에 대한 예측 수행.
6.  **모델 평가 (Model Evaluation)**:
    *   `sklearn.metrics` 모듈의 함수를 사용하여 모델의 성능을 평가.

#### 3. 주요 모듈 소개
*   `sklearn.datasets`: 예제 데이터셋 로드 (e.g., `load_iris()`, `load_digits()`).
*   `sklearn.preprocessing`: 데이터 전처리 기능 (e.g., `StandardScaler`, `MinMaxScaler`, `LabelEncoder`, `OneHotEncoder`).
*   `sklearn.model_selection`: 데이터 분리, 교차 검증, 하이퍼파라미터 튜닝 (e.g., `train_test_split`, `GridSearchCV`, `KFold`).
*   `sklearn.linear_model`: 선형 모델 (e.g., `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`).
*   `sklearn.svm`: 서포트 벡터 머신 (e.g., `SVC`, `SVR`).
*   `sklearn.tree`: 결정 트리 (e.g., `DecisionTreeClassifier`, `DecisionTreeRegressor`).
*   `sklearn.ensemble`: 앙상블 모델 (e.g., `RandomForestClassifier`, `GradientBoostingClassifier`).
*   `sklearn.neighbors`: 최근접 이웃 모델 (e.g., `KNeighborsClassifier`).
*   `sklearn.cluster`: 군집 알고리즘 (e.g., `KMeans`).
*   `sklearn.decomposition`: 차원 축소 (e.g., `PCA`).
*   `sklearn.metrics`: 모델 성능 평가 지표 (e.g., `accuracy_score`, `mean_squared_error`, `confusion_matrix`).

---

### 💻 실습: 붓꽃(Iris) 품종 분류

#### 단계 1: 필요한 라이브러리 및 모듈 임포트
```python
import sklearn
print(sklearn.__version__) # Scikit-learn 버전 확인

# 데이터셋 로드
from sklearn.datasets import load_iris

# 데이터 분리
from sklearn.model_selection import train_test_split

# 모델 (K-최근접 이웃 분류기)
from sklearn.neighbors import KNeighborsClassifier

# 모델 평가
from sklearn.metrics import accuracy_score

import pandas as pd
import numpy as np
```

#### 단계 2: 데이터 준비 및 탐색
```python
# 붓꽃 데이터셋 로드
iris = load_iris()

# iris 객체는 Bunch 클래스 객체로, 딕셔너리와 유사하게 key-value로 구성됨
print("iris 데이터셋의 키:", iris.keys())

# 특성(Features) 데이터 확인
# iris.data는 NumPy 배열
print("\n특성 데이터 (처음 5개 샘플):\n", iris.data[:5])
print("특성 데이터 형태:", iris.data.shape) # (샘플 수, 특성 수)

# 레이블(Labels/Target) 데이터 확인
# iris.target은 NumPy 배열
print("\n레이블 데이터 (처음 5개 샘플):\n", iris.target[:5])
print("레이블 데이터 형태:", iris.target.shape) # (샘플 수,)
print("레이블 이름:", iris.target_names) # ['setosa', 'versicolor', 'virginica']

# 특성 이름 확인
print("\n특성 이름:", iris.feature_names)
# ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

# Pandas DataFrame으로 변환하여 보기 쉽게 (선택 사항)
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label'] = iris.target
print("\nPandas DataFrame (처음 5개 샘플):\n", iris_df.head())
```

#### 단계 3: 훈련 데이터와 테스트 데이터 분리
```python
# 특성 데이터와 레이블 데이터 할당
X = iris.data
y = iris.target

# 훈련 세트와 테스트 세트로 분리 (80% 훈련, 20% 테스트)
# random_state는 재현성을 위해 설정 (동일한 분리 결과를 얻기 위함)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# stratify=y : 원본 데이터의 클래스 비율을 훈련/테스트 세트에도 유지

print("\n훈련 데이터 크기:", X_train.shape, y_train.shape)
print("테스트 데이터 크기:", X_test.shape, y_test.shape)
```

#### 단계 4: 모델 선택 및 학습
```python
# K-최근접 이웃(KNN) 분류기 모델 객체 생성
# n_neighbors는 이웃의 수 (하이퍼파라미터)
knn_model = KNeighborsClassifier(n_neighbors=3)

# 모델 학습 (훈련 데이터 사용)
knn_model.fit(X_train, y_train)
print("\n모델 학습 완료!")
```

#### 단계 5: 모델 예측
```python
# 테스트 데이터로 예측 수행
y_pred = knn_model.predict(X_test)

print("\n테스트 데이터에 대한 예측값 (처음 5개):\n", y_pred[:5])
print("실제 테스트 데이터 레이블 (처음 5개):\n", y_test[:5])
```

#### 단계 6: 모델 평가
```python
# 정확도(Accuracy) 평가
accuracy = accuracy_score(y_test, y_pred)
print(f"\n모델 정확도: {accuracy:.4f}") # 소수점 4자리까지 표시

# 예측 확률 확인 (일부 분류 모델에서 지원)
# 각 클래스(0, 1, 2)에 속할 확률을 반환
if hasattr(knn_model, "predict_proba"):
    y_pred_proba = knn_model.predict_proba(X_test)
    print("\n예측 확률 (처음 1개 샘플):\n", y_pred_proba[0])
```

#### 단계 7: (선택) 새로운 데이터 예측
```python
# 새로운 데이터 예시: [꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비]
new_sample = np.array([[5.0, 3.0, 1.5, 0.2]]) # setosa로 예측될 가능성이 높은 데이터

# 새로운 데이터에 대한 예측
prediction_new = knn_model.predict(new_sample)
predicted_species = iris.target_names[prediction_new[0]]

print(f"\n새로운 샘플 {new_sample} 예측 결과: {predicted_species} (클래스: {prediction_new[0]})")
```

---

### 📝 연습문제

1.  **다른 분류 모델 사용해보기**:
    `sklearn.tree` 모듈의 `DecisionTreeClassifier`를 사용하여 위 붓꽃 분류 문제를 해결하고 정확도를 비교해보세요.
    ```python
    # 예시:
    from sklearn.tree import DecisionTreeClassifier
    # dt_model = DecisionTreeClassifier(random_state=42)
    # dt_model.fit(X_train, y_train)
    # y_pred_dt = dt_model.predict(X_test)
    # accuracy_dt = accuracy_score(y_test, y_pred_dt)
    # print(f"결정 트리 모델 정확도: {accuracy_dt:.4f}")
    ```

2.  **KNN 모델의 하이퍼파라미터 변경해보기**:
    `KNeighborsClassifier`의 `n_neighbors` 값을 1, 5, 10 등으로 변경하면서 정확도가 어떻게 변하는지 관찰해보세요.

3.  **다른 데이터셋 사용해보기**:
    `sklearn.datasets.load_digits()` 데이터셋을 사용하여 숫자(0~9) 분류 문제를 해결해보세요. (특성 스케일링을 고려하면 성능이 더 좋아질 수 있습니다.)

---

### 🎯 과제

1.  **유방암 데이터셋으로 분류 모델링**:
    *   `sklearn.datasets.load_breast_cancer()` 데이터셋을 로드하세요.
    *   데이터를 탐색하고, 훈련 세트와 테스트 세트로 분리하세요.
    *   다음 세 가지 모델을 각각 학습시키고, 테스트 세트에서의 정확도를 비교 분석하세요.
        *   `LogisticRegression` (선형 모델)
        *   `SVC` (서포트 벡터 머신)
        *   `RandomForestClassifier` (앙상블 모델)
    *   (선택) `sklearn.preprocessing.StandardScaler`를 사용하여 특성 스케일링을 적용한 후 모델 성능 변화를 관찰하세요.

---

### ❓ FAQ

1.  **Q: Scikit-learn과 TensorFlow/PyTorch의 차이점은 무엇인가요?**
    A: Scikit-learn은 전통적인 머신러닝 알고리즘(선형 회귀, 로지스틱 회귀, SVM, 결정 트리, 랜덤 포레스트, K-평균 등)에 주로 사용됩니다. 사용하기 쉽고, 다양한 유틸리티 함수를 제공합니다. 반면, TensorFlow와 PyTorch는 주로 딥러닝 모델(신경망)을 구축하고 학습하는 데 특화된 프레임워크입니다. GPU 가속, 자동 미분 등 딥러닝에 필요한 고급 기능을 제공합니다.

2.  **Q: 어떤 머신러닝 모델을 선택해야 할까요?**
    A: 문제의 종류(분류, 회귀, 군집 등), 데이터의 크기와 특성, 모델의 해석 가능성, 학습 시간, 예측 속도 등 다양한 요소를 고려해야 합니다. 처음에는 간단한 모델(예: 로지스틱 회귀, 결정 트리)부터 시작하여 점차 복잡한 모델(예: 랜덤 포레스트, 그래디언트 부스팅, SVM)을 시도해보는 것이 일반적입니다. Scikit-learn은 다양한 모델을 쉽게 교체하며 테스트할 수 있도록 지원합니다. Scikit-learn의 [알고리즘 선택 가이드](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)도 참고할 수 있습니다.

3.  **Q: 데이터에 문자열(범주형) 데이터가 포함되어 있으면 어떻게 하나요?**
    A: 대부분의 Scikit-learn 모델은 숫자형 입력을 기대합니다. 문자열 데이터는 숫자형으로 변환해야 합니다. `sklearn.preprocessing` 모듈의 `LabelEncoder` (순서형 범주)나 `OneHotEncoder` (명목형 범주)를 사용하여 변환할 수 있습니다. Pandas의 `get_dummies()` 함수도 유용합니다.

4.  **Q: 데이터가 너무 커서 메모리에 다 올릴 수 없을 때는 어떻게 하나요?**
    A: Scikit-learn의 일부 모델은 `partial_fit` 메서드를 지원하여 데이터를 조금씩 나누어 학습(점진적 학습, Incremental Learning)할 수 있습니다. 또는 Dask나 Spark와 같은 분산 컴퓨팅 프레임워크와 Scikit-learn을 연동하여 사용하는 방법도 있습니다. 데이터 샘플링을 통해 모델을 학습하는 것도 하나의 방법입니다.

5.  **Q: 더 많은 데이터셋은 어디서 찾을 수 있나요?**
    A: Scikit-learn 내에도 다양한 예제 데이터셋이 있으며 (`sklearn.datasets`), UCI Machine Learning Repository, Kaggle, OpenML 등에서 공개된 다양한 데이터셋을 찾아볼 수 있습니다.
